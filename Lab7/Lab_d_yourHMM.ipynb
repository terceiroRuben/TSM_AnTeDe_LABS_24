{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnTeDe Lab D: Your own HMM PoS tagger\n",
    "\n",
    "## Session goal\n",
    "\n",
    "The goal of this session is build your own PoS tagger. This notebook is based on a programming assignment written by Konstantin Taranov\n",
    "and Ondrej Skopek for a Natural Language Understanding course offered at ETH Zurich in spring 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\fabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown as corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the starting probabilities based on the brown corpus. We also keep track of all the tagged words and tags in the corpus, we will use this information to compute the transition and emission probabilities.\n",
    "\n",
    "**Fill the code to compute the starting probabilities.**\n",
    "\n",
    "_Note: We place START and STOP tags in between the individual sentences in the corpus to avoid mixing things up._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET': 0.21342867108475758, 'NOUN': 0.1411405650505755, 'ADJ': 0.034339030345308684, 'VERB': 0.04513428671084758, 'ADP': 0.1228461806766655, '.': 0.08892570631321939, 'ADV': 0.0913498430415068, 'CONJ': 0.049128008371119636, 'PRT': 0.036675967910708054, 'PRON': 0.15969654691314963, 'NUM': 0.016811998604813395, 'X': 0.0005231949773282176}\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "tagged_words = []\n",
    "all_tags = []\n",
    "\n",
    "starting_p = {}\n",
    "count = 0\n",
    "\n",
    "sentences = corpus.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "for sent in sentences:\n",
    "    tagged_words.append((\"START\", \"START\"))\n",
    "    all_tags.append(\"START\")\n",
    "    start_flag = True\n",
    "    for word, tag in sent:\n",
    "        try:\n",
    "            starting_p[tag]\n",
    "        except:\n",
    "            starting_p[tag] = 0\n",
    "\n",
    "        # BEGIN_YOUR_CODE\n",
    "\n",
    "\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "        all_tags.append(tag)\n",
    "        tagged_words.append((tag, word))\n",
    "    tagged_words.append((\"END\", \"END\"))\n",
    "    all_tags.append(\"END\")\n",
    "\n",
    "for tag in starting_p.keys():\n",
    "    starting_p[tag] = starting_p[tag] / count\n",
    "\n",
    "print(starting_p)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('START', 'START'), ('DET', 'The'), ('NOUN', 'Fulton'), ('NOUN', 'County'), ('ADJ', 'Grand'), ('NOUN', 'Jury'), ('VERB', 'said'), ('NOUN', 'Friday'), ('DET', 'an'), ('NOUN', 'investigation')]\n",
      "['START', 'DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(tagged_words[0:10])\n",
    "print(all_tags[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use NLTK to estimate the transition probabilities and the emission probabilities as conditional probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $P(t_{i} | t_{i-1})= \\frac{C(t_{i-1}, t_{i})}{C(t_{i-1})}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_tags = nltk.ConditionalFreqDist(nltk.bigrams(all_tags))\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADV', 'ADJ') appears 7666  times\n",
      "ADV appears 56239 times\n",
      "The probability of P('ADJ'|'ADV') is roughly 0.1363\n"
     ]
    }
   ],
   "source": [
    "print(\"('ADV', 'ADJ') appears\", cfd_tags[\"ADV\"][\"ADJ\"], \" times\")\n",
    "\n",
    "print(\"ADV appears \" + str(cfd_tags[\"ADV\"].N()) + \" times\")\n",
    "\n",
    "\n",
    "# P('ADJ' | 'ADV')\n",
    "print(\n",
    "    \"The probability of P('ADJ'|'ADV') is roughly\",\n",
    "    round(cpd_tags[\"ADV\"].prob(\"ADJ\"), 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $P(w_{i} | t_{i}) =  \\frac{C(t_{i}, w_{i})}{C(t_{i})}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_tw = nltk.ConditionalFreqDist(tagged_words)\n",
    "cpd_tw = nltk.ConditionalProbDist(cfd_tw, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of C('NOUN', 'dog') is 70\n",
      "Probability of P('dog' | 'NOUN') is 0.0002540300045725401\n"
     ]
    }
   ],
   "source": [
    "# C('dog', 'NOUN'):\n",
    "print(\"Frequency of C('NOUN', 'dog') is\", cfd_tw[\"NOUN\"][\"dog\"])\n",
    "\n",
    "# P('dog' | 'NOUN')\n",
    "print(\"Probability of P('dog' | 'NOUN') is\", cpd_tw[\"NOUN\"].prob(\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows the code for the Viterbi algorithm, adapted from part c.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observations, states, starting_p, transition_p, emission_p):\n",
    "    # your trellis is a list of dictionaries\n",
    "    trellis = [{}]\n",
    "\n",
    "    # first column of the trellis:\n",
    "    # how likely you are to start in each state, multiplied by\n",
    "    # how likely you are to generate the initial observation\n",
    "    # from each state\n",
    "    for state in states:\n",
    "        trellis[0][state] = {\n",
    "            \"probability\": starting_p[state] * emission_p[state][observations[0]],\n",
    "            \"previous state\": None,\n",
    "        }\n",
    "\n",
    "    # for loop over the trellis columns, left to right\n",
    "    for k in range(1, len(observations)):\n",
    "        # add a column\n",
    "        new_column = {}\n",
    "\n",
    "        # for each row in the column\n",
    "        for state in states:\n",
    "            max_path_p = 0\n",
    "\n",
    "            for previous_state in states:\n",
    "                up_to_here_p = (\n",
    "                    trellis[k - 1][previous_state][\"probability\"]\n",
    "                    * transition_p[previous_state][state]\n",
    "                )\n",
    "\n",
    "                if up_to_here_p > max_path_p:\n",
    "                    max_path_p = up_to_here_p\n",
    "\n",
    "                    prev_st_selected = previous_state\n",
    "\n",
    "            max_p = max_path_p * emission_p[state][observations[k]]\n",
    "\n",
    "            new_column[state] = {\"probability\": max_p, \"previous\": prev_st_selected}\n",
    "\n",
    "        trellis.append(new_column)\n",
    "\n",
    "    return trellis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get our transmission probabilities in the same format we had them in part c. Let's check we still have the same P('ADJ'|'ADV') as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']\n",
      "0.13631110083749712\n"
     ]
    }
   ],
   "source": [
    "tagset = list(starting_p.keys())\n",
    "print(tagset)\n",
    "\n",
    "transition_p = {}\n",
    "for tag in tagset:\n",
    "    transition_p[tag] = {}\n",
    "    for conditioned_on_tag in tagset:\n",
    "        transition_p[tag][conditioned_on_tag] = cpd_tags[conditioned_on_tag].prob(tag)\n",
    "\n",
    "\n",
    "print(transition_p[\"ADJ\"][\"ADV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can use your code from 8c to get the PoS tag sequence chosen by your tagger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tag_sequence(trellis, observations):\n",
    "    # BEGIN_YOUR_CODE\n",
    "   \n",
    "   \n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the function that we will call to run our PoS tagger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as tokenize\n",
    "\n",
    "\n",
    "def hmm_postagger(sentence):\n",
    "    observations = tokenize.word_tokenize(sentence)\n",
    "\n",
    "    emission_p = {}\n",
    "    for tag in tagset:\n",
    "        emission_p[tag] = {}\n",
    "        for word in observations:\n",
    "            # fill emmission_p. hint: the emission of tag t for word w is P(w|t)\n",
    "            # BEGIN_YOUR_CODE\n",
    "\n",
    "\n",
    "            # END_YOUR_CODE\n",
    "\n",
    "    my_trellis = viterbi(observations, tagset, starting_p, transition_p, emission_p)\n",
    "    return get_pos_tag_sequence(my_trellis, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our PoS tagger. Come up with other test sentences to see how well it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'DET'), ('dog', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('dog', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"My dog is a really good dog.\"\n",
    "result = hmm_postagger(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
